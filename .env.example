# ===========================================
# DUAL-LLM CONFIGURATION
# ===========================================
# Use different LLMs for planning vs execution
# Planning = initial prompt engineering (quality)
# Execution = iterations/refinements (speed)
USE_DUAL_LLM=false

# Planning LLM (for initial prompt engineering)
PLANNING_LLM_PROVIDER=claude
PLANNING_OLLAMA_MODEL=llama3.1:8b
PLANNING_CLAUDE_MODEL=claude-sonnet-4-5-20250929
PLANNING_GEMINI_MODEL=gemini-2.5-pro

# Execution LLM (for quick iterations)
EXECUTION_LLM_PROVIDER=gemini
EXECUTION_OLLAMA_MODEL=llama3.2:3b
EXECUTION_CLAUDE_MODEL=claude-haiku-4-5-20251001
EXECUTION_GEMINI_MODEL=gemini-2.5-flash

# ===========================================
# SINGLE LLM CONFIGURATION (when USE_DUAL_LLM=false)
# ===========================================
LLM_PROVIDER=ollama

# Ollama Configuration (Local LLM - Free)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
OLLAMA_AUTO_CONFIGURE=true  # Auto-select model based on GPU VRAM

# Anthropic Claude Configuration (Cloud - Requires API Key)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-api-key-here
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# Google Gemini Configuration (Cloud - Requires API Key)
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your-api-key-here
GEMINI_MODEL=gemini-2.5-flash

# Stable Diffusion Automatic1111 API Configuration
SD_API_URL=http://localhost:7860
SD_API_TIMEOUT=300

# CivitAI API Configuration (Optional - for model downloads)
CIVITAI_API_KEY=

# GPU VRAM Detection
# Options: "auto" (detect from SD API), "manual" (user-specified), "disabled" (don't use)
VRAM_DETECTION_MODE=auto
VRAM_MANUAL_GB=8.0  # Used when VRAM_DETECTION_MODE=manual

# Application Configuration
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=True

# Generation defaults
DEFAULT_STEPS=30
DEFAULT_CFG_SCALE=7.0
DEFAULT_WIDTH=512
DEFAULT_HEIGHT=512
DEFAULT_SAMPLER=DPM++ 2M Karras
