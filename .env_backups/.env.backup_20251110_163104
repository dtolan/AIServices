# ===========================================
# DUAL-LLM CONFIGURATION
# ===========================================
# Use different LLMs for planning vs execution
# Planning = initial prompt engineering (quality)
# Execution = iterations/refinements (speed)
USE_DUAL_LLM=True

# Planning LLM (for initial prompt engineering)
PLANNING_LLM_PROVIDER=claude
PLANNING_OLLAMA_MODEL=llama3.1:8b
PLANNING_CLAUDE_MODEL=claude-3-5-sonnet-20241022
PLANNING_GEMINI_MODEL=gemini-1.5-pro

# Execution LLM (for quick iterations)
EXECUTION_LLM_PROVIDER=gemini
EXECUTION_OLLAMA_MODEL=llama3.2:3b
EXECUTION_CLAUDE_MODEL=claude-3-5-haiku-20241022
EXECUTION_GEMINI_MODEL=gemini-1.5-flash

# ===========================================
# SINGLE LLM CONFIGURATION (when USE_DUAL_LLM=false)
# ===========================================
LLM_PROVIDER=ollama

# Ollama Configuration (Local LLM - Free)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
OLLAMA_AUTO_CONFIGURE=True

# Anthropic Claude Configuration (Cloud - Requires API Key)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=*************here
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Google Gemini Configuration (Cloud - Requires API Key)
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=AIzaSyBCG6iWEjiKfjFY7GM8fMKB3RwkmabNrJI
GEMINI_MODEL=gemini-1.5-flash

# Stable Diffusion Automatic1111 API Configuration
SD_API_URL=http://localhost:7860
SD_API_TIMEOUT=300

# Application Configuration
APP_HOST=0.0.0.0
APP_PORT=8000
DEBUG=True

# Generation defaults
DEFAULT_STEPS=30
DEFAULT_CFG_SCALE=7.0
DEFAULT_WIDTH=512
DEFAULT_HEIGHT=512
DEFAULT_SAMPLER=DPM++ 2M Karras
